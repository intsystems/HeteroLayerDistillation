{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T22:20:57.789633Z",
     "start_time": "2022-04-25T22:20:55.750311Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../code')\n",
    "from resnet import *\n",
    "# from funcs import *\n",
    "from cifar_very_tiny import *\n",
    "from cifar_tiny import *\n",
    "from cifar_dataset import *    \n",
    "import torch as t \n",
    "import numpy as np\n",
    "from numpy import polyfit\n",
    "from numpy import polyval\n",
    "import tqdm\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.cm as cm\n",
    "import json\n",
    "# import hyperparams\n",
    "from importlib import reload\n",
    "from scipy.interpolate import interp1d\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize']=(12,9)\n",
    "plt.rcParams['font.size']= 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T22:21:00.802489Z",
     "start_time": "2022-04-25T22:21:00.798491Z"
    }
   },
   "outputs": [],
   "source": [
    "epoch_num = 25\n",
    "# epoch_num = 50\n",
    "\n",
    "run_num = 2 # количество запусков эксперимента\n",
    "\n",
    "# версия нужна, чтобы различать старые и новые результаты экспериментов. \n",
    "# менять нужно каждый раз, когда есть хотя бы незначительные изменения в эксперименте\n",
    "experiment_version = '3'\n",
    "\n",
    "validate_every_epoch = 5 \n",
    "\n",
    "# train_splines_every_epoch = 5 # каждые 5 эпох отслеживать траекторию гиперпараметров\n",
    "# train_splines_every_epoch = 2\n",
    "# train_splines_every_epoch = 3\n",
    "train_splines_every_epoch = 10\n",
    "\n",
    "# размер мини-эпохи в батчах, за которую у нас производится либо обучение спайлов, либо их использование\n",
    "mini_epoch_size = 10\n",
    "\n",
    "start_beta = 0.5\n",
    "start_temp  = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T22:21:04.204382Z",
     "start_time": "2022-04-25T22:21:02.711435Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0c121f92904705ab8243964f5b4201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/cifar-10-python.tar.gz to ../data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader_no_augumentation, valid_loader, test_loader = cifar10_loader(batch_size=128, split_train_val=True,\n",
    "                                                                             maxsize=128*100, use_aug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T22:21:20.346094Z",
     "start_time": "2022-04-25T22:21:20.344061Z"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if t.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T22:21:22.542328Z",
     "start_time": "2022-04-25T22:21:22.524514Z"
    }
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import hyperparams\n",
    "reload(hyperparams)\n",
    "AdamHyperGradCalculator = hyperparams.AdamHyperGradCalculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Student(Cifar_Very_Tiny):\n",
    "    def __init__(self):\n",
    "        Cifar_Very_Tiny.__init__(self, 10)\n",
    "        self.mu_feat = nn.Linear(128, 64).cuda()\n",
    "        self.log_sigma_feat = torch.nn.Parameter(torch.zeros(1).cuda())\n",
    "        self.bad_params = set(self.mu_feat.parameters())\n",
    "        self.bad_params.add(self.log_sigma_feat)\n",
    "        \n",
    "    def inference_parameters(self):\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            if p not in self.bad_params:\n",
    "                yield p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T22:21:25.969608Z",
     "start_time": "2022-04-25T22:21:25.947394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor(1.0000e-04, device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current loss:1.6417592763900757: 100%|██████████| 90/90 [00:05<00:00, 15.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'test loss': 4.4714860916137695, 'val loss': 4.489005088806152, 'accuracy': 0.33239999413490295, 'lambda1': 0.07375209033489227}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current loss:1.2948156595230103: 100%|██████████| 90/90 [00:05<00:00, 15.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'test loss': 4.4714860916137695, 'val loss': 4.489005088806152, 'accuracy': 0.39499998092651367, 'lambda1': 0.16014908254146576}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current loss:1.086913824081421: 100%|██████████| 90/90 [00:05<00:00, 15.68it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'test loss': 4.4714860916137695, 'val loss': 4.489005088806152, 'accuracy': 0.47519999742507935, 'lambda1': 0.2502247989177704}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current loss:0.8874977231025696: 100%|██████████| 90/90 [00:05<00:00, 15.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'test loss': 4.4714860916137695, 'val loss': 4.489005088806152, 'accuracy': 0.4918999969959259, 'lambda1': 0.33169564604759216}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current loss:0.7288898825645447: 100%|██████████| 90/90 [00:05<00:00, 15.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'test loss': 4.306225299835205, 'val loss': 4.311577320098877, 'accuracy': 0.4917999804019928, 'lambda1': 0.4172290563583374}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current loss:0.57944256067276: 100%|██████████| 90/90 [00:05<00:00, 15.53it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 5, 'test loss': 4.306225299835205, 'val loss': 4.311577320098877, 'accuracy': 0.5072999596595764, 'lambda1': 0.5060046911239624}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current loss:0.45569315552711487: 100%|██████████| 90/90 [00:05<00:00, 15.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 6, 'test loss': 4.306225299835205, 'val loss': 4.311577320098877, 'accuracy': 0.524399995803833, 'lambda1': 0.5887020826339722}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current loss:0.3419886529445648: 100%|██████████| 90/90 [00:05<00:00, 15.66it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 7, 'test loss': 4.306225299835205, 'val loss': 4.311577320098877, 'accuracy': 0.5446000099182129, 'lambda1': 0.6726420521736145}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current loss:0.2532210648059845: 100%|██████████| 90/90 [00:05<00:00, 15.67it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 8, 'test loss': 4.306225299835205, 'val loss': 4.311577320098877, 'accuracy': 0.5432000160217285, 'lambda1': 0.7470697164535522}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current loss:0.18000909686088562: 100%|██████████| 90/90 [00:05<00:00, 15.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 9, 'test loss': 4.401458740234375, 'val loss': 4.410752773284912, 'accuracy': 0.5575999617576599, 'lambda1': 0.8105723261833191}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current loss:0.11269327253103256: 100%|██████████| 90/90 [00:05<00:00, 15.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 10, 'test loss': 4.401458740234375, 'val loss': 4.410752773284912, 'accuracy': 0.5745999813079834, 'lambda1': 0.8716939687728882}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current loss:0.06586428731679916: 100%|██████████| 90/90 [00:05<00:00, 15.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 11, 'test loss': 4.401458740234375, 'val loss': 4.410752773284912, 'accuracy': 0.5777999758720398, 'lambda1': 0.9213131070137024}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current loss:0.03842207044363022: 100%|██████████| 90/90 [00:05<00:00, 15.59it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 12, 'test loss': 4.401458740234375, 'val loss': 4.410752773284912, 'accuracy': 0.5798999667167664, 'lambda1': 0.9499425292015076}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current loss:0.02546294406056404: 100%|██████████| 90/90 [00:05<00:00, 15.67it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 13, 'test loss': 4.401458740234375, 'val loss': 4.410752773284912, 'accuracy': 0.5816999673843384, 'lambda1': 0.9634620547294617}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current loss:0.01732959970831871: 100%|██████████| 90/90 [00:05<00:00, 15.66it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 14, 'test loss': 4.388936519622803, 'val loss': 4.402792930603027, 'accuracy': 0.5813999772071838, 'lambda1': 0.9723530411720276}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current loss:0.011713988147675991: 100%|██████████| 90/90 [00:05<00:00, 15.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 15, 'test loss': 4.388936519622803, 'val loss': 4.402792930603027, 'accuracy': 0.5809999704360962, 'lambda1': 0.9782078862190247}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current loss:0.009481444023549557: 100%|██████████| 90/90 [00:05<00:00, 15.32it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 16, 'test loss': 4.388936519622803, 'val loss': 4.402792930603027, 'accuracy': 0.5803999900817871, 'lambda1': 0.9802702069282532}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current loss:0.0058131893165409565: 100%|██████████| 90/90 [00:05<00:00, 15.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 17, 'test loss': 4.388936519622803, 'val loss': 4.402792930603027, 'accuracy': 0.5799999833106995, 'lambda1': 0.9842697978019714}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current loss:0.004792067222297192: 100%|██████████| 90/90 [00:05<00:00, 15.73it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 18, 'test loss': 4.388936519622803, 'val loss': 4.402792930603027, 'accuracy': 0.5796999931335449, 'lambda1': 0.9850409626960754}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current loss:0.004363006912171841: 100%|██████████| 90/90 [00:05<00:00, 15.50it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 19, 'test loss': 4.308137893676758, 'val loss': 4.320822715759277, 'accuracy': 0.5798999667167664, 'lambda1': 0.9850057363510132}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current loss:0.0039780051447451115: 100%|██████████| 90/90 [00:05<00:00, 15.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 20, 'test loss': 4.308137893676758, 'val loss': 4.320822715759277, 'accuracy': 0.5799999833106995, 'lambda1': 0.9855387210845947}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current loss:0.002881210297346115: 100%|██████████| 90/90 [00:05<00:00, 15.33it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 21, 'test loss': 4.308137893676758, 'val loss': 4.320822715759277, 'accuracy': 0.580299973487854, 'lambda1': 0.9866153597831726}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current loss:0.0030536663252860308: 100%|██████████| 90/90 [00:05<00:00, 15.41it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 22, 'test loss': 4.308137893676758, 'val loss': 4.320822715759277, 'accuracy': 0.580299973487854, 'lambda1': 0.9861478209495544}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current loss:0.0041466220282018185: 100%|██████████| 90/90 [00:05<00:00, 15.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 23, 'test loss': 4.308137893676758, 'val loss': 4.320822715759277, 'accuracy': 0.5809999704360962, 'lambda1': 0.9846453070640564}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current loss:0.004894235171377659: 100%|██████████| 90/90 [00:05<00:00, 15.66it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 24, 'test loss': 4.2877984046936035, 'val loss': 4.294524192810059, 'accuracy': 0.5809000134468079, 'lambda1': 0.9836641550064087}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exp_ver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [87]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    134\u001b[0m         out\u001b[38;5;241m.\u001b[39mwrite(json\u001b[38;5;241m.\u001b[39mdumps({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m:internal_results, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m'\u001b[39m: exp_ver})\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m#     # inner function for hyperopt optimization\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m#     return max([res['val acc'] for res in internal_results])\u001b[39;00m\n\u001b[1;32m    138\u001b[0m         \n\u001b[1;32m    139\u001b[0m         \u001b[38;5;66;03m# with open('../logs/acc_mi_'+experiment_version+'.txt','a') as out:\u001b[39;00m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;66;03m#     out.write('{}: {}: {}\\n'.format(lam1, e, ac))\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m \u001b[43mdist_with_opt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_no_augumentation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_every_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambdas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [87]\u001b[0m, in \u001b[0;36mdist_with_opt\u001b[0;34m(experiment_version, train_loader_no_augumentation, test_loader, validation_loader, validate_every_epoch, lambdas, clip_grad, seed)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# if not hyperopt: # outer function optimization\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../logs/acc_mi_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mexperiment_version\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m out:\n\u001b[0;32m--> 134\u001b[0m     out\u001b[38;5;241m.\u001b[39mwrite(json\u001b[38;5;241m.\u001b[39mdumps({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m:internal_results, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mexp_ver\u001b[49m})\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exp_ver' is not defined"
     ]
    }
   ],
   "source": [
    "# определяем функцию потерь как замкнутую относительно аргументов функцию\n",
    "# нужно для подсчета градиентов гиперпараметров по двухуровневой оптимизации\n",
    "def param_loss_mi(batch,model,h):\n",
    "    lam1 = h[0]    \n",
    "    x,y,teacher_feat, teacher_logit = batch        \n",
    "    student_feat, student_logits = model.get_features(x, [3,4])\n",
    "    #print (teacher_feat.shape, teacher_logit.shape, student_feat.shape, student_logits.shape)\n",
    "    class_loss = crit(student_logits, y)\n",
    "    sigma2 = torch.log(1+torch.exp(model.log_sigma_feat))\n",
    "    feat_loss = ((model.mu_feat(teacher_feat) - student_feat)**2).sum(1).mean()/(2*sigma2) + 0.5*torch.log(sigma2)*np.prod(teacher_feat.shape[1:])\n",
    "    loss = class_loss * (1.0-lam1) + feat_loss * lam1 * 1e-4\n",
    "    return loss\n",
    "\n",
    "# определяем функцию валидационную функцию потерь как замкнутую относительно аргументов функцию\n",
    "# нужно для подсчета градиентов гиперпараметров по двухуровневой оптимизации\n",
    "def hyperparam_loss_mi(batch, model):\n",
    "    x,y = batch\n",
    "    student_feat, student_logits = model.get_features(x, [3,4])\n",
    "    class_loss = crit(student_logits, y)             \n",
    "    return class_loss \n",
    "\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "def dist_with_opt(experiment_version, train_loader_no_augumentation, test_loader, validation_loader, validate_every_epoch, lambdas=None, clip_grad=10e-3, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    t.manual_seed(seed)\n",
    "    \n",
    "    # for lam1 in [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 0.5]:\n",
    "    lam1 = t.nn.Parameter(t.tensor(np.random.uniform(low=0.0, high=1.0), device=device), requires_grad=True)\n",
    "    \n",
    "    if lambdas is not None: # non-random initialization\n",
    "        lam1.data *= 0\n",
    "        lam1.data += lambdas[0]\n",
    "\n",
    "    student = Student().cuda()\n",
    "    teacher = Cifar_Tiny(10).cuda() \n",
    "    teacher.load_state_dict(torch.load('../tiny_cifar10.model', map_location=torch.device(device)))\n",
    "    #scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=10, gamma=0.5)   \n",
    "    \n",
    "\n",
    "    #mu_logit = nn.Linear(10, 10).cpu()\n",
    "    #log_sigma_logit = torch.nn.Parameter(torch.zeros(1).cpu())\n",
    "    h =[lam1]\n",
    "    print (lam1)\n",
    "    optim = torch.optim.SGD(list(student.parameters()), lr=1.0)    \n",
    "    scheduler = t.optim.lr_scheduler.StepLR(optim, step_size=10, gamma=0.5)\n",
    "    optim2 = torch.optim.Adam(h, lr=1e-3)   \n",
    "    #net, parameters_loss_function, hyperparameters_loss_function, optimizer, h\n",
    "    hyper_grad_calc = AdamHyperGradCalculator(student, param_loss_mi, hyperparam_loss_mi, optim, h)\n",
    "    val_load_iter = iter(validation_loader)\n",
    "    internal_results = []\n",
    "    for e in range(25):\n",
    "        tq = tqdm.tqdm(train_loader_no_augumentation)\n",
    "        losses = []\n",
    "\n",
    "        for batch_id, (x,y) in enumerate(tq):\n",
    "            try:\n",
    "                 (v_x, v_y) = next(val_load_iter)\n",
    "            except:                    \n",
    "                val_load_iter = iter(validation_loader)\n",
    "                (v_x, v_y) = next(val_load_iter)\n",
    "\n",
    "                \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            v_x = v_x.to(device)\n",
    "            v_y = v_y.to(device)\n",
    "            optim2.zero_grad()\n",
    "            teacher_feat, teacher_logits = teacher.get_features(x, [3,4])\n",
    "            \n",
    "            hyper_grad_calc.calc_gradients((x,y, teacher_feat, teacher_logits), (v_x, v_y))                    \n",
    "            t.nn.utils.clip_grad_value_(h, clip_grad)\n",
    "\n",
    "            for h_ in h:\n",
    "                if h_.grad is not None:\n",
    "                    h_.grad = t.where(t.isnan(h_.grad), t.zeros_like(h_.grad), h_.grad)\n",
    "            \n",
    "            optim2.step()\n",
    "            if lam1 > 1.0:\n",
    "                lam1.data*=0.0\n",
    "                lam1.data+=1.0\n",
    "            if lam1 < 0.0:\n",
    "                lam1.data*=0.0\n",
    "                   \n",
    "            optim.zero_grad()\n",
    "            student_feat, student_logits = student.get_features(x, [3,4])\n",
    "            # class_loss = crit(student_logits, y)\n",
    "            # sigma2 = torch.log(1+torch.exp(log_sigma_feat))\n",
    "            # feat_loss = ((mu_feat(teacher_feat) - student_feat)**2).sum(1).mean()/(2*sigma2) + 0.5*torch.log(sigma2)*np.prod(teacher_feat.shape[1:])\n",
    "            #logit_loss =((mu_feat(teacher_feat) - student_feat)**2).sum(1).mean()/(2*sigma2) + 0.5*torch.log(sigma2)*np.prod(teacher_feat.shape[1:])\n",
    "            #lam1 = 0.5\n",
    "            loss = param_loss_mi((x,y, teacher_feat, teacher_logits), student,h)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            losses.append(loss.detach().cpu().numpy())\n",
    "            tq.set_description('current loss:{}'.format(np.mean(losses[-10:])))      \n",
    "        scheduler.step()\n",
    "        # student.eval()\n",
    "\n",
    "        if e==0 or (e+1)%validate_every_epoch == 0:\n",
    "            test_loss = []\n",
    "            student.eval()\n",
    "            for x,y in test_loader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                student_feat, student_logits = student.get_features(x, [2,3])\n",
    "                test_loss.append(crit(student_logits, y).detach().cpu().numpy())\n",
    "            test_loss = float(np.mean(test_loss))\n",
    "            val_loss = []\n",
    "            for x,y in validation_loader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                student_feat, student_logits = student.get_features(x, [2,3])\n",
    "                val_loss.append(crit(student_logits, y).detach().cpu().numpy())\n",
    "            val_loss = float(np.mean(val_loss))\n",
    "        \n",
    "        ac = float(accuracy(student, test_loader))\n",
    "        student.train()\n",
    "\n",
    "        # if not hyperopt:\n",
    "        internal_results.append({'epoch': e, 'test loss':test_loss, 'val loss':val_loss, 'accuracy':ac,\n",
    "                             'lambda1':float(lam1.cpu().detach().numpy()),\n",
    "                            })\n",
    "        # else:\n",
    "        #     val_acc = float(accuracy(student, validation_loader))\n",
    "        #     internal_results.append({'epoch': e, 'test loss':test_loss, 'val loss':val_loss, 'accuracy':ac,\n",
    "        #                          'lambda1':float(lam1.cpu().detach().numpy()),\n",
    "        #                           'val acc':val_acc})\n",
    "        print (internal_results[-1])\n",
    "\n",
    "    # if not hyperopt: # outer function optimization\n",
    "    with open('../logs/acc_mi_'+experiment_version+'.txt','a') as out:\n",
    "        out.write(json.dumps({'results':internal_results, 'version': exp_ver})+'\\n')\n",
    "    # else:\n",
    "    #     # inner function for hyperopt optimization\n",
    "    #     return max([res['val acc'] for res in internal_results])\n",
    "        \n",
    "        # with open('../logs/acc_mi_'+experiment_version+'.txt','a') as out:\n",
    "        #     out.write('{}: {}: {}\\n'.format(lam1, e, ac))\n",
    "dist_with_opt(experiment_version, train_loader_no_augumentation, test_loader, valid_loader, validate_every_epoch, lambdas=[1e-4])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T22:21:27.709909Z",
     "start_time": "2022-04-25T22:21:27.308140Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(student, t_load):\n",
    "    student.eval()\n",
    "    total = 0 \n",
    "    correct = 0\n",
    "    with t.no_grad():\n",
    "        for x,y in t_load:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            out = student(x)\n",
    "            correct += t.eq(t.argmax(out, 1), y).sum()\n",
    "            total+=len(x)\n",
    "    student.train()\n",
    "    return (correct/total).cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_with_no_opt(experiment_version, train_loader_no_augumentation, test_loader, validation_loader, validate_every_epoch, lambdas=None, file=True, no_tqdm=False, clip_grad=10e-3, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    t.manual_seed(seed)\n",
    "\n",
    "    lam1 = t.nn.Parameter(t.tensor(np.random.uniform(low=0.0, high=1.0), device=device), requires_grad=True)\n",
    "\n",
    "    if lambdas is not None: # non-random initialization\n",
    "        lam1.data *= 0\n",
    "        lam1.data += lambdas[0]\n",
    "    \n",
    "    student = Cifar_Very_Tiny(10).cpu()\n",
    "    teacher = Cifar_Tiny(10).cpu() \n",
    "    teacher.load_state_dict(torch.load('tiny_cifar10.model?raw=true', map_location=torch.device('cpu')))\n",
    "    #scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=10, gamma=0.5)   \n",
    "    mu_feat = nn.Linear(128, 64).cpu()\n",
    "    log_sigma_feat = torch.nn.Parameter(torch.zeros(1).cpu())\n",
    "\n",
    "    #mu_logit = nn.Linear(10, 10).cpu()\n",
    "    #log_sigma_logit = torch.nn.Parameter(torch.zeros(1).cpu())\n",
    "\n",
    "\n",
    "    optim = torch.optim.Adam(list(student.parameters()) + list(mu_feat.parameters()) + [log_sigma_feat])    \n",
    "    val_load_iter = iter(val_load)\n",
    "\n",
    "    for e in range(25):\n",
    "        tq = tqdm.tqdm(train_loader_no_augumentation)\n",
    "        if no_tqdm:\n",
    "            tq = train_loader_no_augumentation\n",
    "        losses = []\n",
    "\n",
    "        for batch_id, (x,y) in enumerate(tq):\n",
    "            try:\n",
    "                 (v_x, v_y) = next(val_load_iter)\n",
    "            except:                    \n",
    "                val_load_iter = iter(val_load)\n",
    "                (v_x, v_y) = next(val_load_iter)\n",
    "                \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optim.zero_grad()\n",
    "            student_feat, student_logits = student.get_features(x, [3,4])\n",
    "            # class_loss = crit(student_logits, y)\n",
    "            # sigma2 = torch.log(1+torch.exp(log_sigma_feat))\n",
    "            # feat_loss = ((mu_feat(teacher_feat) - student_feat)**2).sum(1).mean()/(2*sigma2) + 0.5*torch.log(sigma2)*np.prod(teacher_feat.shape[1:])\n",
    "            #logit_loss =((mu_feat(teacher_feat) - student_feat)**2).sum(1).mean()/(2*sigma2) + 0.5*torch.log(sigma2)*np.prod(teacher_feat.shape[1:])\n",
    "            #lam1 = 0.5\n",
    "            loss = param_loss_mi((x,y,teacher_logits,mu_feat,log_sigma_feat), student,lam1)\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            losses.append(loss.detach().cpu().numpy())\n",
    "            if not no_tqdm:\n",
    "                tq.set_description('current loss:{}'.format(np.mean(losses[-10:])))\n",
    "        #scheduler.step()\n",
    "        \n",
    "        if e==0 or (e+1)%validate_every_epoch == 0:\n",
    "            test_loss = []\n",
    "            student.eval()\n",
    "            for x,y in test_loader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                student_feat, student_logits = student.get_features(x, [3,4])\n",
    "                test_loss.append(crit(student_logits, y).detach().cpu().numpy())\n",
    "            test_loss = float(np.mean(test_loss))\n",
    "            val_loss = []\n",
    "            for x,y in validation_loader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                student_feat, student_logits = student.get_features(x, [3,4])\n",
    "                val_loss.append(crit(student_logits, y).detach().cpu().numpy())\n",
    "            val_loss = float(np.mean(val_loss))\n",
    "        \n",
    "        ac = float(accuracy(student, test_loader))\n",
    "        \n",
    "        if file:\n",
    "            internal_results.append({'epoch': e, 'test loss':test_loss, 'val loss':val_loss, 'accuracy':ac,\n",
    "                                 'lambda1':float(lam1.cpu().detach().numpy()),\n",
    "                                })\n",
    "        else:\n",
    "            val_acc = float(accuracy(student, validation_loader))\n",
    "            internal_results.append({'epoch': e, 'test loss':test_loss, 'val loss':val_loss, 'accuracy':ac,\n",
    "                                 'lambda1':float(lam1.cpu().detach().numpy()),\n",
    "                                  'val acc':val_acc})\n",
    "        \n",
    "        print (internal_results[-1])\n",
    "\n",
    "    if file: # outer function optimization\n",
    "        with open('../logs/acc_mi_'+experiment_version+'.txt','a') as out:\n",
    "            out.write(json.dumps({'results':internal_results, 'version': experiment_version})+'\\n')\n",
    "    else:\n",
    "        # inner function for hyperopt optimization\n",
    "        return max([res['val acc'] for res in internal_results])\n",
    "        \n",
    "        # with open('../logs/acc_mi_'+experiment_version+'.txt','a') as out:\n",
    "        #     out.write('{}: {}: {}\\n'.format(lam1, e, ac))\n",
    "\n",
    "\n",
    "def dist_hyperopt(experiment_version, run_num, tr_load, t_load, val_load, validate_every_epoch, trial_num):\n",
    "    np.random.seed(42)\n",
    "    t.manual_seed(42)\n",
    "\n",
    "    for _ in range(run_num):\n",
    "        cost_function = lambda lambdas: -dist_with_no_opt(experiment_version, train_loader_no_augumentation, test_loader, validation_loader, validate_every_epoch, lambdas = best_lambdas['lambda1'], file=False, no_tqdm=True) # validation accuracy * (-1) -> min\n",
    "       \n",
    "        best_lambdas = fmin(fn=cost_function,                             \n",
    "        #space= [ hp.uniform('lambda1', 0.0, 1.0), hp.uniform('lambda2', 0.0, 1.0), hp.uniform('temp', 0.1, 10.0)],\n",
    "        space= [ hp.uniform('lambda1', 0.0, 1.0)],  \n",
    "        algo=tpe.suggest,\n",
    "        max_evals=trial_num)\n",
    "        #cifar_with_validation_set(exp_ver, 1, epoch_num, filename, tr_s_epoch, m_e, tr_load, t_load, val_load, validate_every_epoch, lambdas = [best_lambdas['lambda1'], best_lambdas['lambda2'], best_lambdas['temp']],  mode='no-opt')\n",
    "        dist_with_no_opt(experiment_version, train_loader_no_augumentation, test_loader, validation_loader, validate_every_epoch, lambdas = best_lambdas['lambda1'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
